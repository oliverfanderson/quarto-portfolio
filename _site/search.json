[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Oliver F. Anderson",
    "section": "",
    "text": "Oliver Faulkner Anderson, MSDS is a data scientist, software engineer, and computational biology researcher at Reed College. As a sports science and nutrition nerd, he found an outlet for these passions while competing in Track & Field as a jumper and sprinter at Willamette University.\nHe presented his undergraduate thesis at the Murdock College Science Research Conference where he developed a pipeline for cleaning and processing data gathered by RNA sequencing. This work was published in the International Journal of Molecular Sciences in collaboration with Reed College. If you would like to find out more about this project, you can check it out along with his other projects here.\n\nExperience\n\nReed College | Portland, OR\nComputational Biology Researcher | Aug 2023 - Current\n\n\nWillamette University | Salem, OR\nChemistry Tutor | Sept 2021 - Dec 2021Research Assistant | May 2021 - August 2021\n\n\nSimple Solutions Landcare LLC | Salem, OR\nCo-Owner & Landscaper | Dec 2017 - July 2019\n\n\n\nEducation\n\nWillamette University | Salem, OR\nMS in Data Science | Sept 2022 - Aug 2023\n\n\nWillamette University | Salem, OR\nBA in Biology | Sept 2016 - May 2022"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "GRPhIN: Graphlet Characterization of Regulatory and Physical Interaction Networks\n\n\n\n\n\n\nacademic\n\n\nalgorithms\n\n\nbiology\n\n\nnetworks\n\n\nvisualization\n\n\nstatistical testing\n\n\nsoftware engineering\n\n\n\nWorking in the Ritz Lab at Reed College, we developed an algorithm to enumerate 2- and 3-node graphlets graphlets and their orbits in a mixed interaction networks.\n\n\n\n\n\nApr 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nProteinWeaver: Visualizing Biological Networks in their Functional Context\n\n\n\n\n\n\nacademic\n\n\nbiology\n\n\nnetworks\n\n\nvisualization\n\n\ndatabases\n\n\ndata engineering\n\n\nsoftware engineering\n\n\n\nUnder the guidance of Dr.Â Anna Ritz at Reed College and in collaboration with Altaf Barelvi, we developed a webtool that enables user to visualize molecular interaction networks in the context of a particular biological function or process.\n\n\n\n\n\nJul 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Biology and Code: Developing a Software Pipeline for Plastic Biodegradation\n\n\n\n\n\n\nacademic\n\n\nbiology\n\n\nanalysis\n\n\ncode\n\n\nbioinformatics\n\n\ndata wrangling\n\n\nbash\n\n\n\n By combining biology and coding, I developed a software pipeline to analyze RNA-seq data and uncover enzymatic pathways for plastic biodegradation. A step towards a greener future.\n\n\n\n\n\nMay 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCity Vacation Planning Shiny App\n\n\n\n\n\n\ninteractive\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nR\n\n\ndata visualization\n\n\nshiny (R package)\n\n\nLeaflet (R package)\n\n\nplotly (R package)\n\n\n\nAn interactive app that helps users plan trips to a selection of cities. Made with the shiny, leaflet, and plotly packages in R. For a better viewing experience, check out the full version here.\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIlluminating Insights: Exploring Interactive Graphics with plotly and ggiraph\n\n\n\n\n\n\nhealth\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nR\n\n\ndata visualization\n\n\n\nDelve into interactive data visualization using plotly and ggiraph in this post. Explore sunshine levels in US cities, analyze intriguing patterns, and uncover potential dream vacation spots.\n\n\n\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSuper Bowl Champions: Analysis Using the gt R Package\n\n\n\n\n\n\nsports\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nR\n\n\ndata visualization\n\n\n\nExplore the Super Bowl championsâ€™ table, highlighting winners, scores, and locations, using the gt R package. Uncover your teamâ€™s success!\n\n\n\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Remote Currency Database: Harnessing the Potential of Docker and API Scraping\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nbash\n\n\ndocker\n\n\nSQL\n\n\nR\n\n\nweb scraping\n\n\ndata engineering\n\n\ndatabases\n\n\n\nDiscover how I created a dynamic currency database in SQL, analyzed GDP growth in R, and uncovered the dollarâ€™s recent strength.\n\n\n\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Map Made with Shiny and Leaflet in R\n\n\n\n\n\n\nhealth & fitness\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nR\n\n\ndata visualization\n\n\nshiny (R package)\n\n\nLeaflet (R package)\n\n\n\nAn interactive map of CDC data on nutrition, exercise, and obesity in the United States. Created in partnership with Hayden Vaughn. For a better viewing experience, check out the full version here.\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUnveiling Body Compositions in Olympic Track and Field: Exploring Data Trends\n\n\n\n\n\n\nhealth & fitness\n\n\nanalysis\n\n\ncode\n\n\ndata wrangling\n\n\nR\n\n\ndata visualization\n\n\n\nVisualization of the body composition of the athletes in different track and field events. Data from #TidyTuesday.\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/super-bowl-winners.html",
    "href": "posts/super-bowl-winners.html",
    "title": "Super Bowl Champions: Analysis Using the gt R Package",
    "section": "",
    "text": "In celebration of the 2023 Super Bowl, I explored the gt R package to analyze the historical data of teams that emerged victorious since the inaugural Super Bowl. Using gt, I created a comprehensive table that presents an overview of Super Bowl champions throughout the years.\nTo begin, I loaded the necessary packages and imported the Super Bowl dataset. I also incorporated team logos into the table for enhanced visual appeal.\nWith the flexibility of the gt package, I curated and formatted the table columns, highlighting essential information such as the Super Bowl number, date, winning and losing teams, points scored, and the eventâ€™s location. This table serves as a valuable resource for gaining insights into the historical context and achievements of each Super Bowl.\n\n\nLook at the code\n# load packages and read in data\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(lubridate)\n\nsuperbowls &lt;- readr::read_csv('./Super_Bowl.csv')\n \n# add logos for each team\nsuperbowls &lt;- superbowls %&gt;% \n  mutate(win_logo_url = \n  case_when(\n          Winner == \"Green Bay Packers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/5/50/Green_Bay_Packers_logo.svg\",\n          Winner == \"New York Jets\" ~ \"https://upload.wikimedia.org/wikipedia/en/6/6b/New_York_Jets_logo.svg\",\n          Winner == \"Kansas City Chiefs\" ~ \"https://upload.wikimedia.org/wikipedia/en/e/e1/Kansas_City_Chiefs_logo.svg\",\n          Winner == \"Baltimore Colts\" ~ \"https://upload.wikimedia.org/wikipedia/en/1/1f/Baltimore_Colts_logo_1961-1978.gif\",\n          Winner == \"Dallas Cowboys\" ~ \"https://upload.wikimedia.org/wikipedia/commons/1/15/Dallas_Cowboys.svg\",\n          Winner == \"Miami Dolphins\" ~ \"https://upload.wikimedia.org/wikipedia/en/3/37/Miami_Dolphins_logo.svg\",\n          Winner == \"Pittsburgh Steelers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/d/de/Pittsburgh_Steelers_logo.svg\",\n          Winner == \"Oakland Raiders\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/48/Las_Vegas_Raiders_logo.svg\",\n          Winner == \"Los Angeles Raiders\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/48/Las_Vegas_Raiders_logo.svg\",\n          Winner == \"San Francisco 49ers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/3/3a/San_Francisco_49ers_logo.svg\",\n          Winner == \"Washington Redskins\" ~ \"https://e7.pngegg.com/pngimages/193/260/png-clipart-redskins-logo-red-skins-logo-sports-nfl-football.png\",\n          Winner == \"Chicago Bears\" ~ \"https://upload.wikimedia.org/wikipedia/commons/5/5c/Chicago_Bears_logo.svg\",\n          Winner == \"New York Giants\" ~ \"https://upload.wikimedia.org/wikipedia/commons/6/60/New_York_Giants_logo.svg\",\n          Winner == \"Denver Broncos\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/44/Denver_Broncos_logo.svg\",\n          Winner == \"St. Louis Rams\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8b/NFL_Rams_logo.svg\",\n          Winner == \"Baltimore Ravens\" ~ \"https://upload.wikimedia.org/wikipedia/en/1/16/Baltimore_Ravens_logo.svg\",\n          Winner == \"New England Patriots\" ~ \"https://upload.wikimedia.org/wikipedia/en/b/b9/New_England_Patriots_logo.svg\",\n          Winner == \"Tampa Bay Buccaneers\" ~ \"https://upload.wikimedia.org/wikipedia/en/a/a2/Tampa_Bay_Buccaneers_logo.svg\",\n          Winner == \"Indianapolis Colts\" ~ \"https://upload.wikimedia.org/wikipedia/commons/0/00/Indianapolis_Colts_logo.svg\",\n          Winner == \"New Orleans Saints\" ~ \"https://upload.wikimedia.org/wikipedia/commons/5/50/New_Orleans_Saints_logo.svg\",\n          Winner == \"Seattle Seahawks\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8e/Seattle_Seahawks_logo.svg\",\n          Winner == \"Philadelphia Eagles\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8e/Philadelphia_Eagles_logo.svg\"\n          )\n       ) %&gt;% \n  mutate(lose_logo_url = \n  case_when(\n          Loser == \"Green Bay Packers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/5/50/Green_Bay_Packers_logo.svg\",\n            Loser == \"Minnesota Vikings\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/48/Minnesota_Vikings_logo.svg\",\n          Loser == \"Kansas City Chiefs\" ~ \"https://upload.wikimedia.org/wikipedia/en/e/e1/Kansas_City_Chiefs_logo.svg\",\n          Loser == \"Baltimore Colts\" ~ \"https://upload.wikimedia.org/wikipedia/en/1/1f/Baltimore_Colts_logo_1961-1978.gif\",\n          Loser == \"Dallas Cowboys\" ~ \"https://upload.wikimedia.org/wikipedia/commons/1/15/Dallas_Cowboys.svg\",\n          Loser == \"Miami Dolphins\" ~ \"https://upload.wikimedia.org/wikipedia/en/3/37/Miami_Dolphins_logo.svg\",\n          Loser == \"Pittsburgh Steelers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/d/de/Pittsburgh_Steelers_logo.svg\",\n          Loser == \"Oakland Raiders\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/48/Las_Vegas_Raiders_logo.svg\",\n          Loser == \"Los Angeles Rams\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8a/Los_Angeles_Rams_logo.svg\",\n          Loser == \"San Francisco 49ers\" ~ \"https://upload.wikimedia.org/wikipedia/commons/3/3a/San_Francisco_49ers_logo.svg\",\n          Loser == \"Washington Redskins\" ~ \"https://e7.pngegg.com/pngimages/193/260/png-clipart-redskins-logo-red-skins-logo-sports-nfl-football.png\",\n          Loser == \"Chicago Bears\" ~ \"https://upload.wikimedia.org/wikipedia/commons/5/5c/Chicago_Bears_logo.svg\",\n          Loser == \"New York Giants\" ~ \"https://upload.wikimedia.org/wikipedia/commons/6/60/New_York_Giants_logo.svg\",\n          Loser == \"Denver Broncos\" ~ \"https://upload.wikimedia.org/wikipedia/en/4/44/Denver_Broncos_logo.svg\",\n          Loser == \"St. Louis Rams\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8b/NFL_Rams_logo.svg\",\n           Loser == \"Cincinnati Bengals\" ~ \"https://upload.wikimedia.org/wikipedia/commons/8/81/Cincinnati_Bengals_logo.svg\",\n          Loser == \"New England Patriots\" ~ \"https://upload.wikimedia.org/wikipedia/en/b/b9/New_England_Patriots_logo.svg\",\n           Loser == \"Buffalo Bills\" ~ \"https://upload.wikimedia.org/wikipedia/en/7/77/Buffalo_Bills_logo.svg\",\n          Loser == \"Indianapolis Colts\" ~ \"https://upload.wikimedia.org/wikipedia/commons/0/00/Indianapolis_Colts_logo.svg\",\n           Loser == \"San Diego Chargers\" ~ \"https://upload.wikimedia.org/wikipedia/en/7/72/NFL_Chargers_logo.svg\",\n          Loser == \"Seattle Seahawks\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8e/Seattle_Seahawks_logo.svg\",\n          Loser == \"Philadelphia Eagles\" ~ \"https://upload.wikimedia.org/wikipedia/en/8/8e/Philadelphia_Eagles_logo.svg\",\n          Loser == \"Atlanta Falcons\" ~ \"https://upload.wikimedia.org/wikipedia/en/c/c5/Atlanta_Falcons_logo.svg\",\n          Loser == \"Tennessee Titans\" ~ \"https://upload.wikimedia.org/wikipedia/en/c/c1/Tennessee_Titans_logo.svg\",\n          Loser == \"Carolina Panthers\" ~ \"https://upload.wikimedia.org/wikipedia/en/1/1c/Carolina_Panthers_logo.svg\",\n          Loser == \"Arizona Cardinals\" ~ \"https://upload.wikimedia.org/wikipedia/en/7/72/Arizona_Cardinals_logo.svg\"\n          )\n       )\n\n# Choosing a subset of columns to display basic information about the Super Bowl\nsuperbowls&lt;-superbowls %&gt;% \n  dplyr::select(\"SB\", \"Date\", \"win_logo_url\", \"Winner\", \"Winning Pts\", \"lose_logo_url\", \"Loser\", \"Losing Pts\", \"City\", \"State\")\n\n# convert to date format\nsuperbowls$Date &lt;- as.Date(superbowls$Date, '%d-%b-%Y') %m+% years(2000) \n# convert to 4 digit year format\nsuperbowls$Date &lt;- dplyr::if_else(superbowls$Date &gt; '2023-01-01', superbowls$Date %m-% years(100), superbowls$Date)\n\n\n# create gt table  \nsuperbowls %&gt;% \n  gt() %&gt;% \n  # add title and subtitle\n  tab_header(\n    title = md(\"**Super Bowl Winners**\"),\n    subtitle = \"Super Bowl I-LII\"\n  ) %&gt;%\n  # format date\n  fmt_date(\n    columns = Date,\n    date_style = 5\n  ) %&gt;%\n  # add winning team label\n  tab_spanner(\n    label = md(\"**Winning Team**\"),\n    columns = c(win_logo_url, Winner, `Winning Pts`)\n  ) %&gt;% \n  # add losing team label\n  tab_spanner(\n    label = \"Losing Team\",\n    columns = c(lose_logo_url, Loser, `Losing Pts`)\n  ) %&gt;% \n  # add location label\n  tab_spanner(\n    label = \"Location\",\n    columns = c(City, State)\n  ) %&gt;% \n  # add super bowl label\n  tab_spanner(\n    label = \"Super Bowl\",\n    columns = c(SB, Date)\n  ) %&gt;% \n  # align the text to center\n  cols_align(\n    align = \"center\") %&gt;% \n  # change names of columns\n  cols_label(\n    SB = \"Number\",\n    Winner = md(\"**Name**\"),\n    `Winning Pts` = md(\"**Points**\"),\n    Loser = \"Name\",\n    `Losing Pts` = \"Points\"\n  ) %&gt;%\n  # convert urls to images\n  text_transform(\n    #Apply a function to a column\n    locations = cells_body(c(win_logo_url, lose_logo_url)),\n    fn = function(x) {\n      #Return an image of set dimensions\n      web_image(\n        url = x,\n        height = 12\n      )\n    }\n  ) %&gt;%\n  #Hide column headers and reduce width\n  cols_width(c(win_logo_url, lose_logo_url) ~ px(30)) %&gt;% \n  cols_label(win_logo_url = \"\") %&gt;% \n  cols_label(lose_logo_url = \"\") %&gt;%\n  # change colors of winning and losing teams\n  tab_style(\n    style = list(\n      cell_fill(\"darkseagreen1\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = c(win_logo_url, Winner, `Winning Pts`)\n    )\n  ) %&gt;% \n  tab_style(\n    style = list(\n      cell_fill(\"#FFADAD\")\n    ),\n    locations = cells_body(\n      columns = c(lose_logo_url, Loser, `Losing Pts`)\n    )\n  ) %&gt;% \n  # add reference footnote\n  tab_source_note(\n    source_note = html(\"Source: &lt;b&gt;Super Bowl Results, Officials, and MVPs&lt;/b&gt;, dataset uploaded by user thedevastator to &lt;a href ='https://www.kaggle.com/datasets/thedevastator/super-bowl-results-officials-and-mvps-1967-2020'&gt;Kaggle.com&lt;/a&gt;.\")\n  )\n\n\n\n\n\n\n\n\nSuper Bowl Winners\n\n\nSuper Bowl I-LII\n\n\n\nSuper Bowl\n\n\nWinning Team\n\n\nLosing Team\n\n\nLocation\n\n\n\nNumber\nDate\n\nName\nPoints\n\nName\nPoints\nCity\nState\n\n\n\n\nI\nJanuary 15, 1967\n\nGreen Bay Packers\n35\n\nKansas City Chiefs\n10\nLos Angeles\nCalifornia\n\n\nII\nJanuary 14, 1968\n\nGreen Bay Packers\n33\n\nOakland Raiders\n14\nMiami\nFlorida\n\n\nIII\nJanuary 12, 1969\n\nNew York Jets\n16\n\nBaltimore Colts\n7\nMiami\nFlorida\n\n\nIV\nJanuary 11, 1970\n\nKansas City Chiefs\n23\n\nMinnesota Vikings\n7\nNew Orleans\nLouisiana\n\n\nV\nJanuary 17, 1971\n\nBaltimore Colts\n16\n\nDallas Cowboys\n13\nMiami\nFlorida\n\n\nVI\nJanuary 16, 1972\n\nDallas Cowboys\n24\n\nMiami Dolphins\n3\nNew Orleans\nLouisiana\n\n\nVII\nJanuary 14, 1973\n\nMiami Dolphins\n14\n\nWashington Redskins\n7\nLos Angeles\nCalifornia\n\n\nVIII\nJanuary 13, 1974\n\nMiami Dolphins\n24\n\nMinnesota Vikings\n7\nHouston\nTexas\n\n\nIX\nJanuary 12, 1975\n\nPittsburgh Steelers\n16\n\nMinnesota Vikings\n6\nNew Orleans\nLouisiana\n\n\nX\nJanuary 18, 1976\n\nPittsburgh Steelers\n21\n\nDallas Cowboys\n17\nMiami\nFlorida\n\n\nXI\nJanuary 9, 1977\n\nOakland Raiders\n32\n\nMinnesota Vikings\n14\nPasadena\nCalifornia\n\n\nXII\nJanuary 15, 1978\n\nDallas Cowboys\n27\n\nDenver Broncos\n10\nNew Orleans\nLouisiana\n\n\nXIII\nJanuary 21, 1979\n\nPittsburgh Steelers\n35\n\nDallas Cowboys\n31\nMiami\nFlorida\n\n\nXIV\nJanuary 20, 1980\n\nPittsburgh Steelers\n31\n\nLos Angeles Rams\n19\nPasadena\nCalifornia\n\n\nXV\nJanuary 25, 1981\n\nOakland Raiders\n27\n\nPhiladelphia Eagles\n10\nNew Orleans\nLouisiana\n\n\nXVI\nJanuary 24, 1982\n\nSan Francisco 49ers\n26\n\nCincinnati Bengals\n21\nPontiac\nMichigan\n\n\nXVII\nJanuary 30, 1983\n\nWashington Redskins\n27\n\nMiami Dolphins\n17\nPasadena\nCalifornia\n\n\nXVIII\nJanuary 22, 1984\n\nLos Angeles Raiders\n38\n\nWashington Redskins\n9\nTampa\nFlorida\n\n\nXIX\nJanuary 20, 1985\n\nSan Francisco 49ers\n38\n\nMiami Dolphins\n16\nPalo Alto\nCalifornia\n\n\nXX\nJanuary 26, 1986\n\nChicago Bears\n46\n\nNew England Patriots\n10\nNew Orleans\nLouisiana\n\n\nXXI\nJanuary 25, 1987\n\nNew York Giants\n39\n\nDenver Broncos\n20\nPasadena\nCalifornia\n\n\nXXII\nJanuary 31, 1988\n\nWashington Redskins\n42\n\nDenver Broncos\n10\nSan Diego\nCalifornia\n\n\nXXIII\nJanuary 22, 1989\n\nSan Francisco 49ers\n20\n\nCincinnati Bengals\n16\nMiami Gardens\nFlorida\n\n\nXXIV\nJanuary 28, 1990\n\nSan Francisco 49ers\n55\n\nDenver Broncos\n10\nNew Orleans\nLouisiana\n\n\nXXV\nJanuary 27, 1991\n\nNew York Giants\n20\n\nBuffalo Bills\n19\nTampa\nFlorida\n\n\nXXVI\nJanuary 26, 1992\n\nWashington Redskins\n37\n\nBuffalo Bills\n24\nMinneapolis\nMinnesota\n\n\nXXVII\nJanuary 31, 1993\n\nDallas Cowboys\n52\n\nBuffalo Bills\n17\nPasadena\nCalifornia\n\n\nXXVIII\nJanuary 30, 1994\n\nDallas Cowboys\n30\n\nBuffalo Bills\n13\nAtlanta\nGeorgia\n\n\nXXIX\nJanuary 29, 1995\n\nSan Francisco 49ers\n49\n\nSan Diego Chargers\n26\nMiami Gardens\nFlorida\n\n\nXXX\nJanuary 28, 1996\n\nDallas Cowboys\n27\n\nPittsburgh Steelers\n17\nTempe\nArizona\n\n\nXXXI\nJanuary 26, 1997\n\nGreen Bay Packers\n35\n\nNew England Patriots\n21\nNew Orleans\nLouisiana\n\n\nXXXII\nJanuary 25, 1998\n\nDenver Broncos\n31\n\nGreen Bay Packers\n24\nSan Diego\nCalifornia\n\n\nXXXIII\nJanuary 31, 1999\n\nDenver Broncos\n34\n\nAtlanta Falcons\n19\nMiami Gardens\nFlorida\n\n\nXXXIV\nJanuary 30, 2000\n\nSt. Louis Rams\n23\n\nTennessee Titans\n16\nAtlanta\nGeorgia\n\n\nXXXV\nJanuary 28, 2001\n\nBaltimore Ravens\n34\n\nNew York Giants\n7\nTampa\nFlorida\n\n\nXXXVI\nFebruary 3, 2002\n\nNew England Patriots\n20\n\nSt. Louis Rams\n17\nNew Orleans\nLouisiana\n\n\nXXXVII\nJanuary 26, 2003\n\nTampa Bay Buccaneers\n48\n\nOakland Raiders\n21\nSan Diego\nCalifornia\n\n\nXXXVIII\nFebruary 1, 2004\n\nNew England Patriots\n32\n\nCarolina Panthers\n29\nHouston\nTexas\n\n\nXXXIX\nFebruary 6, 2005\n\nNew England Patriots\n24\n\nPhiladelphia Eagles\n21\nJacksonville\nFlorida\n\n\nXL\nFebruary 5, 2006\n\nPittsburgh Steelers\n21\n\nSeattle Seahawks\n10\nDetroit\nMichigan\n\n\nXLI\nFebruary 4, 2007\n\nIndianapolis Colts\n29\n\nChicago Bears\n17\nMiami Gardens\nFlorida\n\n\nXLII\nFebruary 3, 2008\n\nNew York Giants\n17\n\nNew England Patriots\n14\nGlendale\nArizona\n\n\nXLIII\nFebruary 1, 2009\n\nPittsburgh Steelers\n27\n\nArizona Cardinals\n23\nTampa\nFlorida\n\n\nXLIV\nFebruary 7, 2010\n\nNew Orleans Saints\n31\n\nIndianapolis Colts\n17\nMiami Gardens\nFlorida\n\n\nXLV\nFebruary 6, 2011\n\nGreen Bay Packers\n31\n\nPittsburgh Steelers\n25\nArlington\nTexas\n\n\nXLVI\nFebruary 5, 2012\n\nNew York Giants\n21\n\nNew England Patriots\n17\nIndianapolis\nIndiana\n\n\nXLVII\nFebruary 3, 2013\n\nBaltimore Ravens\n34\n\nSan Francisco 49ers\n31\nNew Orleans\nLouisiana\n\n\nXLVIII\nFebruary 2, 2014\n\nSeattle Seahawks\n43\n\nDenver Broncos\n8\nEast Rutherford\nNew Jersey\n\n\nXLIX\nFebruary 1, 2015\n\nNew England Patriots\n28\n\nSeattle Seahawks\n24\nGlendale\nArizona\n\n\nL\nFebruary 7, 2016\n\nDenver Broncos\n24\n\nCarolina Panthers\n10\nSanta Clara\nCalifornia\n\n\nLI\nFebruary 5, 2017\n\nNew England Patriots\n34\n\nAtlanta Falcons\n28\nHouston\nTexas\n\n\nLII\nFebruary 4, 2018\n\nPhiladelphia Eagles\n41\n\nNew England Patriots\n33\nMinneapolis\nMinnesota\n\n\n\nSource: Super Bowl Results, Officials, and MVPs, dataset uploaded by user thedevastator to Kaggle.com.\n\n\n\n\n\n\n\n\nAmong the findings, it is disheartening to note that my favorite team, the Cincinnati Bengals, has yet to secure a Super Bowl victory. ðŸ˜¢ Nevertheless, this revelation adds intrigue and further appreciation for the accomplishments of other teams.\nFeel free to explore the table and discover if your team has clinched the coveted Super Bowl title. Join me in commemorating the remarkable achievements, reliving the defining moments, and reflecting on the enduring legacy of the Super Bowl.\nHas your team ever won?"
  },
  {
    "objectID": "posts/olympics-tidytuesday.html",
    "href": "posts/olympics-tidytuesday.html",
    "title": "Unveiling Body Compositions in Olympic Track and Field: Exploring Data Trends",
    "section": "",
    "text": "As a passionate track and field athlete, I have always been curious about the correlation between body composition and performance in various events. In this data science project, I explore the realm of Olympic track and field by utilizing data from the Tidy Tuesday GitHub repository (July 27, 2021) to analyze the body compositions of medalists throughout the years.\n\n\nLook at the code\n# load packages and read in data from tidyTuesday\nlibrary(tidyverse)\nlibrary(colorspace)\nlibrary(patchwork)\nlibrary(DT)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')\n# head(olympics)\n\n# I am interested in looking at track and field (athletics) medalists\nathletics &lt;- olympics %&gt;% \n  filter(sport==\"Athletics\",\n         !is.na(medal))\n\nathletics$sex &lt;- as.factor(athletics$sex)\n\nathletics_colnames &lt;- c(\"ID\", \"Name\", \"Sex\", \"Age\", \"Height\",\n                        \"Weight\", \"Team\", \"NOC\", \"Games\", \"Year\",\n                        \"Season\", \"City\", \"Sport\", \"Event\",\n                        \"Medal\")\n\nDT::datatable(athletics, colnames = athletics_colnames, caption = htmltools::tags$caption(style = \"caption-side: bottom; text-align: center;\", htmltools::strong(\"Table 1: Olympic Track & Field Medalists\")), filter = \"top\", options = list(pageLength = 5, autoWidth = TRUE))\n\n\n\n\n\n\nAfter filtering the data to focus on track and field medalists, I separated the athletes by sex and visualized their height and weight across different events using informative plots.\n\n\nLook at the code\n# initial exploration\nathletics %&gt;% \n  filter(sex==\"F\") %&gt;% \n  ggplot(aes(height, weight, color=event))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nTo simplify the analysis, I selected representative events and grouped them into categories such as Distance, Mid-Distance, Sprints, Jumps, and Throws. I then refined the visualizations, resulting in a cleaner and more understandable graph.\n\n\nLook at the code\n# there are so many events so lets choose a few events that represent some categories\n# distance: 10k, 5k\n# mid-distance: 800m, 1500m\n# sprints: 100m, 400m\n# jumps: long jump, triple jump\n# throws: shot put, discus, javelin\nathletics$event &lt;- gsub(\"Athletics Women's \", \"\", athletics$event)\nathletics$event &lt;- gsub(\"Athletics Men's \", \"\", athletics$event)\n\nathletics &lt;- athletics %&gt;% \n  filter(event == c(\"10,000 metres\", \"5,000 metres\", \"800 metres\",\"1,500 metres\", \"100 metres\", \"400 metres\", \"Long Jump\", \"Triple Jump\", \"Shot Put\", \"Discus Throw\", \"Javelin Throw\"))\n\nathletics %&gt;% \n  ggplot(aes(height, weight, color=event))+\n  geom_point(alpha=0.5)\n\n\n\n\n\n\n\n\n\nLook at the code\n# this is still too much, let's group them\nathletics &lt;- athletics %&gt;% \n  mutate(event_type = case_when( \n  event %in% c(\"10,000 metres\", \"5,000 metres\") ~ \"Distance\",\n  event %in% c(\"800 metres\",\"1,500 metres\") ~ \"Mid-distance\",\n  event %in% c(\"100 metres\", \"400 metres\") ~ \"Sprints\",\n  event %in% c(\"Long Jump\", \"Triple Jump\", \"High Jump\") ~ \"Jumps\",\n  event %in% c(\"Shot Put\", \"Discus Throw\", \"Javelin Throw\") ~ \"Throws\",\n))\n\n# now it's time to subset by sex\nathletics_f &lt;- athletics %&gt;% \n  filter(sex==\"F\")\n\nathletics_m &lt;- athletics %&gt;% \n  filter(sex==\"M\")\n\n# plot female\nathletics_f %&gt;% \n  ggplot(aes(height, weight, color=event_type))+\n  geom_point(alpha=0.5)\n\n\n\n\n\n\n\n\n\nThis is a much easier graphic to understand so I decided to beautify it.\n\n\nLook at the code\n# putting it together\n(( p1 &lt;- athletics_f %&gt;% \n  ggplot(aes(height, weight, color=event_type))+\n  geom_point(alpha=0.75)+\n  theme_minimal()+\n  scale_color_discrete_qualitative(\"Dark 3\")+\n  labs(title=\"Body Composition of Female \\nOlympic Medalists by Event\\n\", x=\"Height (cm)\", y=\"Weight (kg)\", color=\"Type of Track & Field Event\") ))\n\n\n\n\n\n\n\n\n\nThe findings reveal fascinating trends. Female throwers generally exhibit larger body compositions, both in terms of weight and height, while distance runners tend to be smaller and lighter compared to other athletes. Jumpers, on the other hand, show either tall stature (likely due to high jumpers) or body compositions similar to sprinters. Short sprinters demonstrate a balanced distribution without extreme skewness in either height or weight, while mid-distance runners appear slightly lighter and shorter than their short sprinter counterparts. The sprinters, as a group, exhibit similar body types, with an average height of around 170 cm and weight of 60 kg.\n\n\nLook at the code\n# putting it together\n(( p2 &lt;- athletics_m %&gt;% \n  ggplot(aes(height, weight, color=event_type))+\n  geom_point(alpha=0.75)+\n  theme_minimal()+\n  scale_color_discrete_qualitative(\"Dark 3\")+\n  labs(title=\"Body Composition of Male \\nOlympic Medalists by Event\\n\", x=\"Height (cm)\", y=\"Weight (kg)\", color=\"Type of Track & Field Event\") ))\n\n\n\n\n\n\n\n\n\nSimilar patterns emerge among male Olympic medalists. Throwers dominate the upper right quadrant, indicating their tendency to be taller and heavier than other athletes. Like their female counterparts, male distance runners exhibit lighter and shorter body compositions. Jumpers and sprinters showcase comparable body types, with mid-distance runners falling between short sprinters and distance runners.\nInterestingly, I was surprised to discover that throwers tend to be significantly taller than other athletes, as I initially assumed jumpers would be the tallest, given high jumpersâ€™ requirements. The realization that distance runners tend to be shorter, which hadnâ€™t caught my attention previously, suggests the importance of the height-to-weight ratio. Being lighter appears advantageous for distance runners, and a shorter stature contributes to achieving this goal.\nThese data align with my observations of track and field events. The trends indicate that specific events attract athletes with distinct body sizes, likely due to inherent advantages associated with their builds. This exploration sheds light on the relationship between body composition and performance, further deepening our understanding of the sport.\nThis project has been illuminating, combining my passion for track and field with data science techniques. Please reach out if you have intriguing project ideas or would like to collaborate. Letâ€™s embark on compelling data-driven journeys together, unraveling new insights into whatever intrigues you."
  },
  {
    "objectID": "posts/sunshine-interactive-visuals.html",
    "href": "posts/sunshine-interactive-visuals.html",
    "title": "Illuminating Insights: Exploring Interactive Graphics with plotly and ggiraph",
    "section": "",
    "text": "In this post, I explore interactive data visualization using the R packages plotly and ggiraph, featuring a dataset on sunshine levels across the U.S. sourced from Kaggle.\nI begin with plotly, creating an interactive line plot that highlights cities of personal significance. Users can double-click cities in the legend to isolate their data for deeper analysis.\nNext, I use ggiraph to build a bar chart comparing cities with similar sunshine levels. An interesting anomaly caught my attention â€”- Los Angeles shows a dip in sunshine during May and June compared to April and July. This raised questions and I aim to investigate further to understand if this pattern is widespread or unique.\nExperimenting with these R packages and the sunshine dataset was rewarding. I hope these visualizations inspire you to explore new destinations for your next sunny getaway.\n\n\nLook at the code\n# load packages and read in data\nlibrary(tidyverse)\nlibrary(ggiraph)\nlibrary(plotly)\nlibrary(ggpubr)\nlibrary(patchwork)\n\nsunshine &lt;- readr::read_csv('./avg_sunshine.csv')\n\n# get cities into the right format\nsunshine$CITY &lt;- str_to_title(sunshine$CITY)\nsunshine$CITY &lt;- str_replace(sunshine$CITY, \",\", \", \")\nsunshine$CITY &lt;- gsub(\"(\\\\w$)\", \"\\\\U\\\\1\", sunshine$CITY, perl = TRUE)\n\n# get rid of duplicates\nsunshine &lt;- sunshine %&gt;% \n  filter(CITY != \"CitY\")\n\nsunshine &lt;- sunshine[!duplicated(sunshine$CITY), ]\n\n# get data into the right format\nsunshine &lt;- pivot_longer(sunshine, JAN:DEC, names_to = \"month\", values_to = \"temp\")\n\nsunshine &lt;- sunshine[, -4]\nsunshine$ANN&lt;-as.numeric(sunshine$ANN)\nsunshine$temp&lt;-as.numeric(sunshine$temp)\nsunshine$month&lt;-str_to_title(sunshine$month)\nsunshine$month&lt;-as.factor(sunshine$month)\nsunshine$perc_temp&lt;-sunshine$temp/100\n\n\nsun_cities &lt;- sunshine %&gt;% \n  filter(CITY %in% c(\"Portland, OR\", \"Los Angeles, CA\", \"Honolulu, HI\", \"Chicago, IL\", \"Boston, MA\"))\n\nsun_cities$month &lt;- factor(sun_cities$month, levels=c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"))\n\n\n\n# basic line plot for ggiraph\ngg_sunshine &lt;- sun_cities %&gt;%  \n  ggplot(aes(x = month, y=temp, text = paste0(\"Percent possible sunshine in\\n\",CITY, \": \", temp, \"% in \", month))) +\n  geom_line(aes(x = month, y = temp, color = CITY, group = CITY), alpha=0.8)+\n  labs(x=\"Month\", y=\"Average percent of possible sunshine\", color = \"City\", caption = \"Data from Kaggle.com: uploaded by user thedevastator\", title = \"How sunny are the cities that \\nare important to me?\", subtitle = \"Measured by time of sunshine reaching earth from sunrise to sunset\")+\n  theme_minimal()+\n  theme(\n        plot.title = element_text(vjust = 2),\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+\n  scale_y_continuous(labels = scales::percent_format(scale = 1))+\n  scale_color_manual(values=c(\"black\",\"#0454a4\", \"#5688c1\",\"#7ca4d4\", \"#b8cee6\"))\n\nggplotly(gg_sunshine, tooltip = 'text')\n\n\n\n\n\n\nLook at the code\n# working on the ggiraph\n\ntooltip_css &lt;- \"background-color:#7ca4d4;color:white;padding:5px;border-radius:3px;\"\n\navg_sun &lt;- sunshine %&gt;% group_by(CITY, ANN) %&gt;% summarize() %&gt;% na.omit()\n\n# Creating plots\navg_sun$tooltip &lt;- c(paste0(avg_sun$CITY,\": \", avg_sun$ANN, \"%\"))\n\nsun1 &lt;- avg_sun[1:38,] %&gt;% \n  ggplot()+\n  geom_col_interactive(aes(x=reorder(CITY, ANN), y=ANN/100,\n                           tooltip = tooltip,\n                      data_id = ANN), fill=\"#0454a4\")+\n  coord_flip()+\n    labs(x=\"\", y=\"Average annual sunshine\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(vjust = 2))+\n  lims(y=c(0,1))+\n  scale_y_continuous(labels = scales::percent)+\n  scale_x_discrete(labels=NULL)\n\n\nsun2&lt;-avg_sun[39:76,] %&gt;% \n  ggplot()+\n  geom_col_interactive(aes(x=reorder(CITY, ANN), y=ANN/100,\n                           tooltip = tooltip,\n                      data_id = ANN), fill=\"#0454a4\")+\n  coord_flip()+\n    labs(x=\"\", y=\"Average annual sunshine\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(vjust = 2))+\n  lims(y=c(0,1))+\n  scale_y_continuous(labels = scales::percent)+\n  scale_x_discrete(labels=NULL)\n\nsun3&lt;-avg_sun[77:115,] %&gt;% \n  ggplot()+\n  geom_col_interactive(aes(x=reorder(CITY, ANN), y=ANN/100,\n                           tooltip = tooltip,\n                      data_id = ANN), fill=\"#0454a4\")+\n  coord_flip()+\n    labs(x=\"\", y=\"Average annual sunshine\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(vjust = 2))+\n  lims(y=c(0,1))+\n  scale_y_continuous(labels = scales::percent)+\n  scale_x_discrete(labels=NULL)\n\nsun4&lt;-avg_sun[116:153,] %&gt;% \n  ggplot()+\n  geom_col_interactive(aes(x=reorder(CITY, ANN), y=ANN/100,\n                           tooltip = tooltip,\n                      data_id = ANN), fill=\"#0454a4\")+\n  coord_flip()+\n    labs(x=\"\", y=\"Average annual sunshine\")+\n  theme_minimal()+\n  theme(axis.text.x = element_text(vjust = 2))+\n  lims(y=c(0,1))+\n  scale_y_continuous(labels = scales::percent)+\n  scale_x_discrete(labels=NULL)\n\n # sun_plots &lt;- ggarrange(\n #  sun1, sun2, sun3, sun4,\n #  common.legend = TRUE,\n #  legend = \"right\"\n #  )\n p&lt;-ggarrange(sun1, sun2, sun3, sun4, ncol=2, nrow=2, labels=c(\"A\",\"B\",\"C\",\"D\"))\ngirafe(\n  code = print(p + plot_annotation(title=\"How sunny is your hometown?\", subtitle = \"Measured by time of sun shine reaching earth from sunrise to sunset\", caption = \"A: Abilene TX - Des Moines, IA\\nB: Detroit, MI - Las Vegas, NV\\nC: Lihue, HI - Providence, RI\\nD: Pueblo, CO - Yap- W Caroline Is., PC\\nData from Kaggle.com: uploaded by user thedevastator\", theme = theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust = 0)))),\n  height_svg = 9,\n  width_svg = NULL,\n  options = list(\n    opts_tooltip(css = tooltip_css, opacity = 1),\n    opts_sizing(width = .7),\n    opts_hover(css = \"fill:#0454a4;stroke-width:2;\"),\n    opts_hover_inv(css = \"opacity:0.1;\"),\n    opts_selection(\n      type = \"single\", \n      only_shiny = FALSE,\n      css = \"fill:#0454a4\"),\n    opts_zoom(max=4)\n  )\n)"
  },
  {
    "objectID": "posts/city-vacation-planner-shiny.html",
    "href": "posts/city-vacation-planner-shiny.html",
    "title": "City Vacation Planning Shiny App",
    "section": "",
    "text": "&lt;p&gt;Shiny App&lt;/p&gt;"
  },
  {
    "objectID": "posts/proteinweaver.html",
    "href": "posts/proteinweaver.html",
    "title": "ProteinWeaver: Visualizing Biological Networks in their Functional Context",
    "section": "",
    "text": "ProteinWeaver is a tool that answers the simple question: How does a particular protein connect to a particular pathway or process? To accomplish this, ProteinWeaver visualizes molecular interaction networks in the context of a user-specified Gene Ontology term. To learn more about ProteinWeaver you can visit the official ProteinWeaver website or the GitHub repository.\n\n\n\nFig. 1 - Screenshot of an example ProteinWeaver query.\n\n\nI am grateful for the opportunity to have worked with Altaf Barelvi and Dr.Â Anna Ritz on this project."
  },
  {
    "objectID": "contact.html#email-icons-png-designed-by-iyikon-photo-by-tyler-lastovich",
    "href": "contact.html#email-icons-png-designed-by-iyikon-photo-by-tyler-lastovich",
    "title": "Contact Me",
    "section": "email icons PNG Designed By IYIKONPhoto by Tyler Lastovich",
    "text": "email icons PNG Designed By IYIKONPhoto by Tyler Lastovich"
  },
  {
    "objectID": "posts/obesity-rates-shiny.html",
    "href": "posts/obesity-rates-shiny.html",
    "title": "Interactive Map Made with Shiny and Leaflet in R",
    "section": "",
    "text": "&lt;p&gt;Shiny App&lt;/p&gt;"
  },
  {
    "objectID": "posts/currency-rates-database.html",
    "href": "posts/currency-rates-database.html",
    "title": "Creating a Remote Currency Database: Harnessing the Potential of Docker and API Scraping",
    "section": "",
    "text": "Join me on a journey as I delve into database creation, web scraping, and insightful analysis. In this project, I focused on currency exchange rates relative to USD and constructed a robust database by daily scraping from an API. Leveraging Docker containers on Railway.app, I established a remote database for seamless access and comprehensive analysis.\nTo enhance the database, I integrated GDP data by scraping and linking it with currency rates. Additionally, I sourced country names and the currency codes to establish meaningful connections. By importing historical data spanning from the time of GDP growth measurement to my initial API scrape, I gained deeper insights for analysis.\n\n\n\n\nFig 1. - Entity relationship diagram for currency database. Primary keys and linking relationships are shown.\n\n\nWithin the SQL database, I engineered a dynamic View that automatically updated with average monthly currency rates. Equipped with these powerful resources, I conducted an in-depth analysis to identify the fastest growing economies of 2017. However, itâ€™s crucial to acknowledge that the results were influenced by the recent strength of the dollar. Many currencies exhibited relative underperformance against USD, underscoring the robustness of the dollar in recent times. Notably, the Euroâ€™s performance, often aligned with the dollar, further illuminated the strengthening trend.\n\n\n\n\nFig 2. - Exchange rates compared to USD for the fastest growing economies of 2017. The US dollar appears to perform well relative to other currencies.\n\n\nThe process of scraping, database creation, and analysis was both enjoyable and rewarding. As a passionate data engineer, I thrive on solving puzzles and ensuring data aligns perfectly with my objectives. I eagerly anticipate undertaking more data engineering projects, pushing boundaries, and uncovering new insights.\nIf you have compelling project ideas or wish to collaborate, please donâ€™t hesitate to reach out to me. Letâ€™s embark on captivating data-driven adventures together, unraveling new perspectives in the dynamic world of data engineering."
  },
  {
    "objectID": "posts/grphin.html",
    "href": "posts/grphin.html",
    "title": "GRPhIN: Graphlet Characterization of Regulatory and Physical Interaction Networks",
    "section": "",
    "text": "GRPhIN (Graphlet Characterization of Regulatory and Physical Interaction Networks) is an algorithm for counting graphlets and the specific node positions within each graphlet (called orbits) in mixed regulatory and physical interaction networks. Graph representions of regulatory or physical interactions in isolation may obscure the complete functional context of a protein. PPI networks and GRNs do not exist separately; proteins are transcription factors, genes encode proteins, and physical and regulatory interactions mix and coexist forming their own distinct patterns. Graphlets are small, connected, induced subnetworks that describe patterns, local topologies, and organization in networks.\nGRPhIN takes as input (1) an undirected PPI network and (2) a directed regulatory network and counts all mixed graphlets and their respective orbits (Figure 6). GRPhIN provides additional functional context to the roles a protein may play beyond traditional isolated network types.\nLearn more about GRPhIN at the GitHub Repository or read the preprint here. GRPhIN was selected as a full length talk at GLBIO 2025.\n\n\n\n\nHow graphlet sorting works in GRPhIN\n\n\n\nFig. 1 - Orbit labeling and graphlet sorting. A) Unique edges and orbit in an RPI graph (orbit 0 indicates a non-edge; orbits 1â€“7 are mapped to the five distinct edge types). B) Sorting node orbits in ascending order maintains a unique graphlet structure. C) Sorting isomorphisms result in the same unique set of integer pairs.\n\n\nI am grateful for the opportunity to have worked with Altaf Barelvi and Dr.Â Anna Ritz on this project and get hands on experience developing algorithms."
  },
  {
    "objectID": "posts/rna-seq-data-wrangling.html",
    "href": "posts/rna-seq-data-wrangling.html",
    "title": "Integrating Biology and Code: Developing a Software Pipeline for Plastic Biodegradation",
    "section": "",
    "text": "I had the incredible opportunity to combine my passion for biology and coding in my undergraduate thesis project. Working alongside Dr.Â Rosa LeÃ³n-Zayas, Grace Sheehan, and Reed College, we aimed to identify enzymes involved in the biodegradation of plastic, specifically polyethylene terephthalate (PET). By sequencing the RNA of bacteria exposed to plastic, we analyzed gene expression and translated it into protein and enzymatic counterparts.\nTo ensure high-quality data, we developed an optimized software pipeline for cleaning and analyzing RNA-seq data. Despite a data loss in the control group, we successfully identified abnormal gene expression patterns and generated metabolic pathways for each bacterium in our consortium. Through extensive database searches, we discovered previously unknown enzymes that potentially contribute to PET degradation.\n\n\n\nFig. 1 - Flowchart of cleaning RNA-seq data and identifying enzymatic pathways. Created with Biorender.com.\n\n\nI proposed a hypothetical enzymatic pathway for PET degradation, which marked a significant step towards combating global pollution. Our findings were presented at the Murdock College Science Research Conference and later published in the International Journal of Molecular Sciences. This research opens doors for the development of natural waste treatment facilities where bacteria biodegrade plastic waste, and potentially even genetically engineer bacteria to produce plastic polymers from waste materials.\n\n\n\nFig. 2 - Hypothetical enzymatic pathway that I identified. A more thorough enzymatic pathway has been identified in the publication. Created with Biorender.com.\n\n\nI am grateful for the collaboration with Dr.Â Rosa LeÃ³n-Zayas, Grace Sheehan, and our colleagues at Reed College, as well as the support from the NSF and the Murdock Trust. This project holds immense promise for a sustainable future, where biology and technology work hand in hand to address environmental challenges."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, Iâ€™m Oliver!",
    "section": "",
    "text": "Iâ€™m Oliver F. Anderson, a Data Scientist and Health and Fitness Nerd. Get to know me better.\nTake a look around and check out some of the projects that I have been working on."
  },
  {
    "objectID": "index.html#photo-by-xue-guangjian",
    "href": "index.html#photo-by-xue-guangjian",
    "title": "Hello, Iâ€™m Oliver!",
    "section": "Photo by Xue Guangjian",
    "text": "Photo by Xue Guangjian"
  }
]